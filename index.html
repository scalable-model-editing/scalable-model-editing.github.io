<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" href="style.css">
    <title>Research Projects</title>
    <style>
        .twittertop {
            display: block; /* Change to display: block; to show */
        }
    </style>
</head>

<body>
    <nav>
        <ul>
            <div class="nav-container">
                <li class="nav-logo1"><img src="UC Berkeley.png" alt="Logo 2"></li>
                <li class="nav-logo"><img src="BAIR.png" alt="Logo 1"></li>
                
            </div>
            <div class="nav-links">
                <li><a href="papers.html">List of Papers</a></li>
                <li><a href="#news">News</a></li>
                <li><a href="team.html">Team</a></li>
                <li><a href="learn_model_editing.html">Model Editing Resources</a></li>
            </div>
        </ul>
        <div class="title-separator"></div>
    </nav>
    <div class="title">
        <header>
            <h1>Large Scale Reliable Model Editing</h1>
        </header>
    </div>

    <div class="container">
        <section id="introduction">
            <p>As knowledge gets outdated frequently and new facts are created everyday, model editing helps keep models up-to-date without the expensive procedure of pre-training and relying solely on using retrieval based methods. In this project, we focus on model editing methods that modify the parameters of language models. Current model editing methods are successfuly able to perform singular knowledge edits with great precision, but <a href="paper1/index.html">lead to significant model degradation as they are scaled.</a> In this project, we have the following aims:  </p>
            <ol>
                <li><b>Large Scale Model Editing</b> - Creating better sequential and batched editing methods as well as <a href="paper4/index.html">analyzing existing methods in these settings.</a> </li>
                <li><b>Improving Edit Quality</b> - Current model editing methods fail to generalize to logical continuations of the edited fact as well as transfer to other languages made by the model. We aim to improve the quality of model edits along these lines both at small and large scale.</li>
                <li><b>Model Interpretability</b> - Model editing requires a better understanding of the knowledge storing mechanisms in LLMs and in the process of solving this problem, we also hope to make these models more interpretable.</li>
                <li><b>Creating Continually Learning and Editable Models</b> - Our moonshot goal is to create continually learning and editable models. </li>
            </ol>
        </section>
        
        <section id="news">
            <h2 class="news-title">News</h2>
            <ul>
                <li>May 15, 2024 - <a href="paper1/index.html">"Model Editing at Scale: Impacts on Gradual and Catastrophic Forgetting"</a> accepted to <b>ACL 2024 Findings</b></li>
                <li>May 2, 2024 - <a href="paper4/index.html">"Is Bigger Edit Batch Size Always Better?- An Empirical Study on Model Editing with Llama-3"</a> picked by <a href="https://twitter.com/_akhaliq/status/1785869426924966190"> AK</a> as part of <a href="https://huggingface.co/papers/2405.00664"> Huggingface Daily Papers</a></li>
                <li>May 1, 2024 - <a href="paper4/index.html">"Is Bigger Edit Batch Size Always Better?- An Empirical Study on Model Editing with Llama-3"</a> released on arxiv</li>
                <li>March 21, 2024 - <a href="paper3/index.html">"A Unified Framework for Model Editing"</a> released on arxiv</li>
                <li>March 11, 2024 - <a href="paper2/index.html">"Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing"</a> released on arxiv</li>
                <li>January 15, 2024 - <a href="paper1/index.html">"Model Editing at Scale leads to Gradual and Catastrophic Forgetting"</a> released on arxiv</li>
            </ul>
        </section>
    </div>

    <p>&emsp; Website Credits - <a href="https://www.linkedin.com/in/ashok-devireddy/"> Ashok Devireddy</a>, <a href="https://www.linkedin.com/in/maochuan-lu-734574232/"> Maochuan Lu</a></p>
    <footer>
        <p>&copy; 2024 UC Berkeley. All rights reserved.</p>
    </footer>
</body>
</html>
